{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import toimage\n",
    "from keras.datasets import cifar10\n",
    "import keras.utils as Kutils\n",
    "import keras.backend as K\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# 毎回乱数のseedを同じにしておく(実験の再現のため)\n",
    "np.random.seed(120671)        \n",
    "\n",
    "# import my functions\n",
    "from triplet_generator import TripletGenerator, make_triplet_loss_func, bpr_triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend import set_session,tensorflow_backend\n",
    "config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)) #利用量に合わせて確保\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.49 # 上限を0.49に抑える\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_dir = '../exp/'\n",
    "log_dir = '../logs/cifar10_tripletloss-online'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 1)\n",
      "(10000, 32, 32, 3) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "nclasses = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 入力画像の次元\n",
    "img_rows, img_cols = 32, 32\n",
    "\n",
    "# チャネル数（RGBなので3）\n",
    "img_channels = 3\n",
    "\n",
    "# CIFAR-10データをロード\n",
    "# (nb_samples, nb_rows, nb_cols, nb_channel) = tf\n",
    "nb_cols = nb_rows = 32\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# 画素値を0-1に変換\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "# クラスラベル（0-9）をone-hotエンコーディング形式に変換\n",
    "y_train = Kutils.to_categorical(y_train, nclasses)\n",
    "y_test = Kutils.to_categorical(y_test, nclasses)\n",
    "\n",
    "# trainをtrainとvalidationに分割\n",
    "def shuffle_arrays(arrays, random_seed=np.random.randint(100000)):\n",
    "    for ar in arrays:        \n",
    "        np.random.seed(random_seed)        \n",
    "        np.random.shuffle(ar)\n",
    "shuffle_arrays([X_train,y_train])\n",
    "\n",
    "batch_size=480\n",
    "n_train_samples = 45120\n",
    "n_val_samples = 4800\n",
    "X_val = X_train[-n_val_samples:]\n",
    "y_val = y_train[-n_val_samples:]\n",
    "X_train = X_train[:n_train_samples]\n",
    "y_train = y_train[:n_train_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation,Conv2D,MaxPooling2D,Dropout,Flatten,BatchNormalization, Input, Lambda\n",
    "from keras import regularizers\n",
    "\n",
    "# make a submodel\n",
    "# オリジナルのcifar10用モデルからの変更点\n",
    "# 1. BatchNormalizationをしてみる．\n",
    "# 2. loss関数をtriplet lossにしてみる.\n",
    "#temp = Conv2D(32, (3, 3), padding=\"same\", input_shape=X_train.shape[1:]))\n",
    "inputs = Input(shape=(32, 32, 3))\n",
    "temp = Conv2D(32, (3, 3), padding=\"same\",name=\"Conv1\")(inputs)\n",
    "temp = Activation('relu')(temp)\n",
    "\n",
    "temp = Conv2D(32, (3, 3),name=\"Conv2\")(temp)\n",
    "temp = BatchNormalization(name=\"BN2\")(temp)\n",
    "temp = Activation('relu',name=\"Relu2\")(temp)\n",
    "temp = MaxPooling2D(pool_size=(2, 2),name=\"Pool2\")(temp)\n",
    "temp = Dropout(0.25,name=\"DO2\")(temp)\n",
    "\n",
    "temp = Conv2D(64, (3, 3), padding='same',name=\"Conv3\")(temp)\n",
    "temp = BatchNormalization(name=\"BN3\")(temp)\n",
    "temp = Activation('relu',name=\"Relu3\")(temp)\n",
    "temp = Conv2D(64, (3, 3),name=\"Conv4\")(temp)\n",
    "temp = BatchNormalization(name=\"BN4\")(temp)\n",
    "temp = Activation('relu',name=\"Relu4\")(temp)\n",
    "temp = MaxPooling2D(pool_size=(2, 2),name=\"Pool4\")(temp)\n",
    "temp = Dropout(0.25,name=\"DO4\")(temp)\n",
    "\n",
    "temp = Flatten(name=\"Flatten5\")(temp)\n",
    "temp = Dense(512,name=\"Dense5\")(temp)\n",
    "temp = Activation('relu',name=\"Relu5\")(temp)\n",
    "feature = Dropout(0.5,name=\"DO5\")(temp)\n",
    "\n",
    "# wrap up the layers into a single layer-like object\n",
    "feature = Lambda(lambda x: K.l2_normalize(x,axis=-1),name=\"l2normalization\")(temp)\n",
    "\n",
    "_submodel = Model(inputs,feature,name=\"cifar10_convnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare main inputs\n",
    "samples = Input(shape=(32,32,3), name=\"samples\")\n",
    "submodel = _submodel(samples)\n",
    "\n",
    "# add layers for prediction\n",
    "linear_classifier = Dense(nclasses, name=\"linear_classifier\")(submodel)\n",
    "predictions = Activation('softmax',name=\"predictions\")(linear_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "model4triplet = Model(inputs=[samples], outputs=[submodel])\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "\n",
    "num_cats = len(np.unique(y_train, axis=0))\n",
    "base_batch_size = 80\n",
    "num_base_step = len(X_train) / base_batch_size\n",
    "n_triplets_exp = ((base_batch_size/num_cats)**2)*num_cats\n",
    "\n",
    "val_datagen = ImageDataGenerator()\n",
    "val_gen= val_datagen.flow(X_val,y_val,batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make triplet loss functions\n",
    "alpha=0.2 # param for triplet loss.\n",
    "model4triplet.compile(loss=make_triplet_loss_func(alpha),optimizer='adam')\n",
    "tri_gen = TripletGenerator(datagen.flow(X_train,y_train, batch_size=base_batch_size), model4triplet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# test code.\\ny= np.array([[0,1],[0,1],[0,1],[1,0],[1,0]])\\nX = np.random.rand(5,2)\\n#X = np.array([[1.0,0.5],[0.2,0.3],[0.4,0.1],[0.7,0.5],[0.8,0.5]])\\ntriplets,triplets_y = tri_gen.get_triplets(X,y,embeddings=X)\\n\\ntest_feature = K.constant(triplets) \\ntest_y = K.constant(triplets_y)\\n#print(triplets)\\n#print(triplets_y)\\n#loss = tri_gen.triplet_loss(test_y,test_feature)\\n\\n\\nloss = tri_gen.triplet_loss(test_y,test_feature)\\nprint(K.eval(loss))\\nloss = tri_gen.bpr_triplet_loss(test_y,test_feature)\\nprint(K.eval(loss))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# test code.\n",
    "y= np.array([[0,1],[0,1],[0,1],[1,0],[1,0]])\n",
    "X = np.random.rand(5,2)\n",
    "#X = np.array([[1.0,0.5],[0.2,0.3],[0.4,0.1],[0.7,0.5],[0.8,0.5]])\n",
    "triplets,triplets_y = tri_gen.get_triplets(X,y,embeddings=X)\n",
    "\n",
    "test_feature = K.constant(triplets) \n",
    "test_y = K.constant(triplets_y)\n",
    "#print(triplets)\n",
    "#print(triplets_y)\n",
    "#loss = tri_gen.triplet_loss(test_y,test_feature)\n",
    "\n",
    "\n",
    "loss = tri_gen.triplet_loss(test_y,test_feature)\n",
    "print(K.eval(loss))\n",
    "loss = tri_gen.bpr_triplet_loss(test_y,test_feature)\n",
    "print(K.eval(loss))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.callbacks\n",
    "\n",
    "def train_triplet_loss(epochs, steps_per_epoch=int(num_base_step * (n_triplets_exp/batch_size)) / 10):\n",
    "    model = Model(inputs=[samples], outputs=[submodel])\n",
    "    model.compile(loss=make_triplet_loss_func(alpha),optimizer='adam')\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    fpath = \"%s/weights_tripletloss_only.{epoch:02d}.hdf5'\"%log_dir\n",
    "\n",
    "    cp_cb = keras.callbacks.ModelCheckpoint(fpath, monitor='val_predictions_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=100)\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    return model.fit_generator(tri_gen.triplet_flow(batch_size),\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        validation_data=val_gen,\n",
    "                        validation_steps=10, \n",
    "                        epochs=epochs,\n",
    "                        callbacks=[cp_cb],\n",
    "                               )\n",
    "\n",
    "def train_cross_entropy(epochs,steps_per_epoch=n_train_samples/batch_size):\n",
    "    model = Model(inputs=[samples], outputs=[predictions])\n",
    "\n",
    "    model.compile(loss={'predictions':'categorical_crossentropy'},\n",
    "                  optimizer='adam', \n",
    "                  metrics={'predictions':'accuracy'})\n",
    "    model.summary()\n",
    "\n",
    "    # steps_per_epoch should be (number of training images total / batch_size) \n",
    "    # validation_steps should be (number of validation images total / batch_size) \n",
    "    fpath = \"%s/weights_crossentropy_only.{epoch:02d}.hdf5'\"%log_dir\n",
    "    cp_cb = keras.callbacks.ModelCheckpoint(\n",
    "        fpath,\n",
    "        monitor='val_predictions_loss', \n",
    "        verbose=1, \n",
    "        save_best_only=False, save_weights_only=False, mode='auto', period=5)\n",
    "\n",
    "    train_flow = datagen.flow(X_train[:45120],y_train[:45120], batch_size=batch_size)\n",
    "\n",
    "    return model.fit_generator(train_flow,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        validation_data=val_gen,\n",
    "                        validation_steps=10, \n",
    "                        epochs=epochs,\n",
    "                        callbacks=[cp_cb],\n",
    "                               )\n",
    "\n",
    "def train_mix(epochs, loss_weights,  steps_per_epoch=int(num_base_step * (n_triplets_exp/batch_size)) / 10):\n",
    "    class flow_wrapper:\n",
    "        def __init__(self,baseflow):\n",
    "            self.baseflow = baseflow\n",
    "        def flow(self):\n",
    "            while True:\n",
    "                yield self._generator()\n",
    "        def _generator(self):\n",
    "            X,y = next(self.baseflow)\n",
    "            return X,[y,y]\n",
    "\n",
    "    model = Model(inputs=[samples], outputs=[predictions, submodel])\n",
    "    model.compile(loss={'predictions':'categorical_crossentropy','cifar10_convnet':make_triplet_loss_func(alpha)},\n",
    "                  optimizer='adam', loss_weights=loss_weights,\n",
    "                  metrics={'predictions':'accuracy'})\n",
    "    model.summary()\n",
    "\n",
    "    fpath = \"%s/weights_tripletloss_crossentropy.{epoch:02d}.hdf5'\"%log_dir\n",
    "\n",
    "    cp_cb = keras.callbacks.ModelCheckpoint(fpath, monitor='val_predictions_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=100)\n",
    "    warnings.filterwarnings('ignore')\n",
    "    return model.fit_generator(flow_wrapper(tri_gen.triplet_flow(batch_size)).flow(),\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        validation_data=flow_wrapper(val_gen).flow(),\n",
    "                        validation_steps=10, \n",
    "                        epochs=epochs,\n",
    "                        callbacks=[cp_cb],\n",
    "                               )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "def eval_density(X,y, model4triplet):\n",
    "    def _eval_density(X,y, model4triplet):\n",
    "        embeddings = model4triplet.predict(X)\n",
    "        dist_mat = pairwise_distances(embeddings, metric='sqeuclidean', n_jobs=multiprocessing.cpu_count())\n",
    "        #print(dist_mat\n",
    "        _sum = np.sum(dist_mat,axis=None)\n",
    "        n_samples = len(X)\n",
    "        return _sum/(n_samples*(n_samples-1))\n",
    "\n",
    "    cats = np.unique(y, axis=0)\n",
    "    for c in cats:\n",
    "        c_samples = [np.all(a==b) for a,b in zip(y,[c]*len(y))]\n",
    "        X_ = X[c_samples]\n",
    "        y_ = y[c_samples]\n",
    "        del c_samples\n",
    "        avg_dist = _eval_density(X_,y_,model4triplet)\n",
    "        print(\"cat: %02d avg. l2 dist: %0.5f\"%(np.where(c)[0][0],avg_dist))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "samples (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "cifar10_convnet (Model)      (None, 512)               1246368   \n",
      "_________________________________________________________________\n",
      "linear_classifier (Dense)    (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "predictions (Activation)     (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,251,498\n",
      "Trainable params: 1,251,178\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      " 7/94 [=>............................] - ETA: 445s - loss: 1.7226 - acc: 0.4220"
     ]
    }
   ],
   "source": [
    "train_cross_entropy(1)\n",
    "eval_density(X_train,y_train,model4triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_mix(1,[0.5,1.0])\n",
    "eval_density(X_train,y_train,model4triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_triplet_loss(1)\n",
    "eval_density(X_train,y_train,model4triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
